"""
ä¸»ç®¡æ¶æ„ç³»ç»Ÿ - Supervisory Architecture
å°† Deep Researcher ä½œä¸ºå­ agentï¼Œç”±ä¸»ç®¡æ™ºèƒ½ä½“åè°ƒç®¡ç†
"""

import asyncio
from typing import Dict, List, Any, Optional, Literal
from dataclasses import dataclass
from enum import Enum
from pydantic import BaseModel, Field

from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END

from Langgraph_deep_researcher.graph import graph as deep_researcher_graph
from Langgraph_deep_researcher.state import SummaryStateInput
from Langgraph_deep_researcher.configuration import Configuration


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    RESEARCH = "research"
    ANALYSIS = "analysis"
    SYNTHESIS = "synthesis"
    VALIDATION = "validation"


class AgentStatus(Enum):
    """Agent çŠ¶æ€æšä¸¾"""
    IDLE = "idle"
    BUSY = "busy"
    COMPLETED = "completed"
    ERROR = "error"


@dataclass
class Task:
    """ä»»åŠ¡æ•°æ®ç»“æ„"""
    id: str
    type: TaskType
    description: str
    priority: int = 1  # 1-5, 5ä¸ºæœ€é«˜ä¼˜å…ˆçº§
    assigned_agent: Optional[str] = None
    status: str = "pending"
    result: Optional[str] = None
    metadata: Dict[str, Any] = None


class SupervisoryState(BaseModel):
    """ä¸»ç®¡æ™ºèƒ½ä½“çŠ¶æ€"""
    # ç”¨æˆ·è¾“å…¥
    user_request: str = Field(description="ç”¨æˆ·çš„åŸå§‹è¯·æ±‚")
    
    # ä»»åŠ¡ç®¡ç†
    tasks: List[Task] = Field(default_factory=list, description="ä»»åŠ¡åˆ—è¡¨")
    current_task_index: int = Field(default=0, description="å½“å‰ä»»åŠ¡ç´¢å¼•")
    
    # Agent çŠ¶æ€
    deep_researcher_status: AgentStatus = Field(default=AgentStatus.IDLE)
    analysis_agent_status: AgentStatus = Field(default=AgentStatus.IDLE)
    
    # ç»“æœå­˜å‚¨
    research_results: List[str] = Field(default_factory=list)
    analysis_results: List[str] = Field(default_factory=list)
    final_synthesis: str = Field(default="")
    
    # å¯¹è¯å†å²
    messages: List[BaseMessage] = Field(default_factory=list)
    
    # é…ç½®
    config: Dict[str, Any] = Field(default_factory=dict)


class SupervisoryStateInput(BaseModel):
    """ä¸»ç®¡æ™ºèƒ½ä½“è¾“å…¥"""
    user_request: str


class SupervisoryStateOutput(BaseModel):
    """ä¸»ç®¡æ™ºèƒ½ä½“è¾“å‡º"""
    final_synthesis: str
    research_results: List[str]
    analysis_results: List[str]


class SupervisoryAgent:
    """ä¸»ç®¡æ™ºèƒ½ä½“ - è´Ÿè´£ä»»åŠ¡åˆ†è§£ã€åˆ†é…å’Œåè°ƒ"""
    
    def __init__(self, config: Configuration, verbose: bool = False):
        self.config = config
        self.verbose = verbose
        self.llm = self._get_llm()
        
    def _print_progress(self, message: str, level: str = "INFO"):
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        if self.verbose:
            icons = {
                "INFO": "â„¹ï¸",
                "SUCCESS": "âœ…", 
                "WARNING": "âš ï¸",
                "ERROR": "âŒ",
                "PROGRESS": "ğŸ”„",
                "TASK": "ğŸ“‹",
                "RESEARCH": "ğŸ”",
                "ANALYSIS": "ğŸ§ ",
                "SYNTHESIS": "ğŸ“"
            }
            icon = icons.get(level, "â„¹ï¸")
            print(f"{icon} {message}")
        
    def _get_llm(self):
        """è·å– LLM å®ä¾‹"""
        if self.config.llm_provider == "openai":
            return ChatOpenAI(
                model=self.config.local_llm,
                base_url=self.config.openai_base_url,
                temperature=0.1
            )
        else:
            return ChatOllama(
                model=self.config.local_llm,
                base_url=self.config.ollama_base_url,
                temperature=0.1
            )
    
    def decompose_request(self, user_request: str) -> List[Task]:
        """å°†ç”¨æˆ·è¯·æ±‚åˆ†è§£ä¸ºå…·ä½“ä»»åŠ¡"""
        self._print_progress(f"å¼€å§‹åˆ†è§£ç”¨æˆ·è¯·æ±‚: {user_request}", "TASK")
        
        decomposition_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»»åŠ¡åˆ†è§£ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„å¤æ‚è¯·æ±‚åˆ†è§£ä¸ºå…·ä½“çš„ã€å¯æ‰§è¡Œçš„ä»»åŠ¡ã€‚

ç”¨æˆ·è¯·æ±‚: {user_request}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä»»åŠ¡åˆ—è¡¨ï¼ˆJSONæ ¼å¼ï¼‰:
{{
    "tasks": [
        {{
            "id": "task_1",
            "type": "research",
            "description": "ç ”ç©¶ä»»åŠ¡æè¿°",
            "priority": 5
        }},
        {{
            "id": "task_2", 
            "type": "analysis",
            "description": "åˆ†æä»»åŠ¡æè¿°",
            "priority": 4
        }}
    ]
}}

ä»»åŠ¡ç±»å‹è¯´æ˜:
- research: éœ€è¦æ·±åº¦ç ”ç©¶æ”¶é›†ä¿¡æ¯çš„ä»»åŠ¡
- analysis: éœ€è¦åˆ†æã€æ¯”è¾ƒã€è¯„ä¼°çš„ä»»åŠ¡
- synthesis: éœ€è¦ç»¼åˆæ•´ç†çš„ä»»åŠ¡
- validation: éœ€è¦éªŒè¯ã€æ£€æŸ¥çš„ä»»åŠ¡

è¯·ç¡®ä¿ä»»åŠ¡åˆ†è§£åˆç†ã€å…·ä½“ã€å¯æ‰§è¡Œã€‚
"""

        messages = [
            SystemMessage(content=decomposition_prompt),
            HumanMessage(content=f"è¯·åˆ†è§£è¿™ä¸ªè¯·æ±‚: {user_request}")
        ]
        
        try:
            self._print_progress("æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œä»»åŠ¡åˆ†è§£...", "PROGRESS")
            response = self.llm.invoke(messages)
            self._print_progress("æ­£åœ¨è§£æä»»åŠ¡åˆ†è§£ç»“æœ...", "PROGRESS")
            
            # è¿™é‡Œéœ€è¦è§£æ JSON å“åº”å¹¶åˆ›å»º Task å¯¹è±¡
            # ç®€åŒ–å®ç°ï¼Œç›´æ¥åˆ›å»ºç¤ºä¾‹ä»»åŠ¡
            tasks = [
                Task(
                    id="research_task",
                    type=TaskType.RESEARCH,
                    description=f"æ·±åº¦ç ”ç©¶: {user_request}",
                    priority=5
                ),
                Task(
                    id="analysis_task",
                    type=TaskType.ANALYSIS,
                    description=f"åˆ†æç ”ç©¶ç»“æœå¹¶ç”Ÿæˆè§è§£",
                    priority=4
                ),
                Task(
                    id="synthesis_task",
                    type=TaskType.SYNTHESIS,
                    description="ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
                    priority=3
                )
            ]
            
            self._print_progress(f"æˆåŠŸåˆ†è§£ä¸º {len(tasks)} ä¸ªä»»åŠ¡", "SUCCESS")
            for i, task in enumerate(tasks, 1):
                self._print_progress(f"  ä»»åŠ¡ {i}: {task.type.value} - {task.description[:50]}...", "INFO")
            
            return tasks
        except Exception as e:
            self._print_progress(f"ä»»åŠ¡åˆ†è§£å¤±è´¥: {e}", "ERROR")
            self._print_progress("ä½¿ç”¨é»˜è®¤ä»»åŠ¡åˆ†è§£ç­–ç•¥", "WARNING")
            # è¿”å›é»˜è®¤ä»»åŠ¡
            return [
                Task(
                    id="research_task",
                    type=TaskType.RESEARCH,
                    description=f"ç ”ç©¶: {user_request}",
                    priority=5
                )
            ]


class DeepResearcherAgent:
    """Deep Researcher å­ Agent"""
    
    def __init__(self, config: Configuration, verbose: bool = False):
        self.config = config
        self.verbose = verbose
        self.status = AgentStatus.IDLE
        
    def _print_progress(self, message: str, level: str = "INFO"):
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        if self.verbose:
            icons = {
                "INFO": "â„¹ï¸",
                "SUCCESS": "âœ…", 
                "WARNING": "âš ï¸",
                "ERROR": "âŒ",
                "PROGRESS": "ğŸ”„",
                "TASK": "ğŸ“‹",
                "RESEARCH": "ğŸ”",
                "ANALYSIS": "ğŸ§ ",
                "SYNTHESIS": "ğŸ“"
            }
            icon = icons.get(level, "â„¹ï¸")
            print(f"{icon} [DeepResearcher] {message}")
    
    async def execute_research(self, task: Task) -> str:
        """æ‰§è¡Œç ”ç©¶ä»»åŠ¡"""
        self.status = AgentStatus.BUSY
        self._print_progress(f"å¼€å§‹æ‰§è¡Œç ”ç©¶ä»»åŠ¡: {task.description}", "RESEARCH")
        
        try:
            self._print_progress("æ­£åœ¨åˆå§‹åŒ–Deep Researcher...", "PROGRESS")
            # ä½¿ç”¨ç°æœ‰çš„ Deep Researcher
            input_state = SummaryStateInput(research_topic=task.description)
            config_dict = {
                "configurable": {
                    "max_web_research_loops": self.config.max_web_research_loops,
                    "llm_provider": self.config.llm_provider,
                    "local_llm": self.config.local_llm,
                    "search_api": self.config.search_api
                }
            }
            
            self._print_progress("æ­£åœ¨è¿è¡ŒDeep Researcherç ”ç©¶æµç¨‹...", "PROGRESS")
            result = deep_researcher_graph.invoke(input_state, config=config_dict)
            research_result = result.get("running_summary", "ç ”ç©¶å¤±è´¥")
            
            self._print_progress("ç ”ç©¶ä»»åŠ¡å®Œæˆ", "SUCCESS")
            self.status = AgentStatus.COMPLETED
            return research_result
            
        except Exception as e:
            self._print_progress(f"ç ”ç©¶ä»»åŠ¡å¤±è´¥: {e}", "ERROR")
            self.status = AgentStatus.ERROR
            return f"ç ”ç©¶æ‰§è¡Œå¤±è´¥: {str(e)}"


class AnalysisAgent:
    """åˆ†æ Agent - è´Ÿè´£åˆ†æç ”ç©¶ç»“æœ"""
    
    def __init__(self, config: Configuration, verbose: bool = False):
        self.config = config
        self.verbose = verbose
        self.status = AgentStatus.IDLE
        self.llm = self._get_llm()
        
    def _print_progress(self, message: str, level: str = "INFO"):
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        if self.verbose:
            icons = {
                "INFO": "â„¹ï¸",
                "SUCCESS": "âœ…", 
                "WARNING": "âš ï¸",
                "ERROR": "âŒ",
                "PROGRESS": "ğŸ”„",
                "TASK": "ğŸ“‹",
                "RESEARCH": "ğŸ”",
                "ANALYSIS": "ğŸ§ ",
                "SYNTHESIS": "ğŸ“"
            }
            icon = icons.get(level, "â„¹ï¸")
            print(f"{icon} [AnalysisAgent] {message}")
    
    def _get_llm(self):
        """è·å– LLM å®ä¾‹"""
        if self.config.llm_provider == "openai":
            return ChatOpenAI(
                model=self.config.local_llm,
                base_url=self.config.openai_base_url,
                temperature=0.2
            )
        else:
            return ChatOllama(
                model=self.config.local_llm,
                base_url=self.config.ollama_base_url,
                temperature=0.2
            )
    
    async def analyze_results(self, research_results: List[str], original_request: str) -> str:
        """åˆ†æç ”ç©¶ç»“æœ"""
        self.status = AgentStatus.BUSY
        self._print_progress(f"å¼€å§‹åˆ†æ {len(research_results)} ä¸ªç ”ç©¶ç»“æœ", "ANALYSIS")
        
        try:
            self._print_progress("æ­£åœ¨æ„å»ºåˆ†ææç¤º...", "PROGRESS")
            analysis_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆ†æä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ç ”ç©¶ç»“æœè¿›è¡Œæ·±åº¦åˆ†æ:

åŸå§‹è¯·æ±‚: {original_request}

ç ”ç©¶ç»“æœ:
{chr(10).join(research_results)}

è¯·æä¾›:
1. å…³é”®å‘ç°å’Œæ´å¯Ÿ
2. è¶‹åŠ¿åˆ†æ
3. ä¼˜ç¼ºç‚¹è¯„ä¼°
4. æœªæ¥å±•æœ›
5. å®ç”¨å»ºè®®

è¯·ç”¨ç»“æ„åŒ–çš„æ–¹å¼ç»„ç»‡ä½ çš„åˆ†æã€‚
"""

            self._print_progress("æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œåˆ†æ...", "PROGRESS")
            messages = [
                SystemMessage(content=analysis_prompt),
                HumanMessage(content="è¯·åˆ†æè¿™äº›ç ”ç©¶ç»“æœ")
            ]
            
            response = self.llm.invoke(messages)
            analysis_result = response.content
            
            self._print_progress("åˆ†æä»»åŠ¡å®Œæˆ", "SUCCESS")
            self.status = AgentStatus.COMPLETED
            return analysis_result
            
        except Exception as e:
            self._print_progress(f"åˆ†æä»»åŠ¡å¤±è´¥: {e}", "ERROR")
            self.status = AgentStatus.ERROR
            return f"åˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}"


class SynthesisAgent:
    """ç»¼åˆ Agent - è´Ÿè´£ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    
    def __init__(self, config: Configuration, verbose: bool = False):
        self.config = config
        self.verbose = verbose
        self.status = AgentStatus.IDLE
        self.llm = self._get_llm()
        
    def _print_progress(self, message: str, level: str = "INFO"):
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        if self.verbose:
            icons = {
                "INFO": "â„¹ï¸",
                "SUCCESS": "âœ…", 
                "WARNING": "âš ï¸",
                "ERROR": "âŒ",
                "PROGRESS": "ğŸ”„",
                "TASK": "ğŸ“‹",
                "RESEARCH": "ğŸ”",
                "ANALYSIS": "ğŸ§ ",
                "SYNTHESIS": "ğŸ“"
            }
            icon = icons.get(level, "â„¹ï¸")
            print(f"{icon} [SynthesisAgent] {message}")
    
    def _get_llm(self):
        """è·å– LLM å®ä¾‹"""
        if self.config.llm_provider == "openai":
            return ChatOpenAI(
                model=self.config.local_llm,
                base_url=self.config.openai_base_url,
                temperature=0.3
            )
        else:
            return ChatOllama(
                model=self.config.local_llm,
                base_url=self.config.ollama_base_url,
                temperature=0.3
            )
    
    async def synthesize_final_report(self, 
                                    research_results: List[str], 
                                    analysis_results: List[str],
                                    original_request: str) -> str:
        """ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š"""
        self.status = AgentStatus.BUSY
        self._print_progress(f"å¼€å§‹ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š", "SYNTHESIS")
        
        try:
            self._print_progress("æ­£åœ¨æ„å»ºç»¼åˆæŠ¥å‘Šæç¤º...", "PROGRESS")
            synthesis_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ¥å‘Šæ’°å†™ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€ä»½å®Œæ•´çš„ç»¼åˆæŠ¥å‘Š:

åŸå§‹è¯·æ±‚: {original_request}

ç ”ç©¶ç»“æœ:
{chr(10).join(research_results)}

åˆ†æç»“æœ:
{chr(10).join(analysis_results)}

è¯·ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„æœ€ç»ˆæŠ¥å‘Šï¼ŒåŒ…æ‹¬:
1. æ‰§è¡Œæ‘˜è¦
2. è¯¦ç»†åˆ†æ
3. å…³é”®å‘ç°
4. ç»“è®ºå’Œå»ºè®®
5. å‚è€ƒèµ„æ–™

æŠ¥å‘Šåº”è¯¥ä¸“ä¸šã€å…¨é¢ã€æ˜“è¯»ã€‚
"""

            self._print_progress("æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆç»¼åˆæŠ¥å‘Š...", "PROGRESS")
            messages = [
                SystemMessage(content=synthesis_prompt),
                HumanMessage(content="è¯·ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š")
            ]
            
            response = self.llm.invoke(messages)
            final_report = response.content
            
            self._print_progress("ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ", "SUCCESS")
            self.status = AgentStatus.COMPLETED
            return final_report
            
        except Exception as e:
            self._print_progress(f"ç»¼åˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}", "ERROR")
            self.status = AgentStatus.ERROR
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"


# LangGraph èŠ‚ç‚¹å‡½æ•°
def decompose_request_node(state: SupervisoryState, config: RunnableConfig) -> Dict[str, Any]:
    """ä»»åŠ¡åˆ†è§£èŠ‚ç‚¹"""
    supervisory_config = Configuration.from_runnable_config(config)
    verbose = config.get("configurable", {}).get("verbose", False)
    supervisory_agent = SupervisoryAgent(supervisory_config, verbose=verbose)
    
    supervisory_agent._print_progress("å¼€å§‹ä»»åŠ¡åˆ†è§£é˜¶æ®µ", "TASK")
    tasks = supervisory_agent.decompose_request(state.user_request)
    
    return {
        "tasks": tasks,
        "current_task_index": 0,
        "messages": state.messages + [AIMessage(content=f"å·²å°†è¯·æ±‚åˆ†è§£ä¸º {len(tasks)} ä¸ªä»»åŠ¡")]
    }


def execute_research_node(state: SupervisoryState, config: RunnableConfig) -> Dict[str, Any]:
    """æ‰§è¡Œç ”ç©¶èŠ‚ç‚¹"""
    supervisory_config = Configuration.from_runnable_config(config)
    verbose = config.get("configurable", {}).get("verbose", False)
    research_agent = DeepResearcherAgent(supervisory_config, verbose=verbose)
    
    # æ‰¾åˆ°ç ”ç©¶ä»»åŠ¡
    research_tasks = [task for task in state.tasks if task.type == TaskType.RESEARCH]
    
    if not research_tasks:
        return {
            "deep_researcher_status": AgentStatus.ERROR,
            "research_results": [],
            "messages": state.messages + [AIMessage(content="æœªæ‰¾åˆ°ç ”ç©¶ä»»åŠ¡")]
        }
    
    # æ‰§è¡Œç ”ç©¶ä»»åŠ¡
    research_results = []
    for task in research_tasks:
        result = asyncio.run(research_agent.execute_research(task))
        research_results.append(result)
    
    return {
        "deep_researcher_status": AgentStatus.COMPLETED,
        "research_results": research_results,
        "messages": state.messages + [AIMessage(content=f"å®Œæˆ {len(research_results)} ä¸ªç ”ç©¶ä»»åŠ¡")]
    }


def analyze_results_node(state: SupervisoryState, config: RunnableConfig) -> Dict[str, Any]:
    """åˆ†æç»“æœèŠ‚ç‚¹"""
    supervisory_config = Configuration.from_runnable_config(config)
    verbose = config.get("configurable", {}).get("verbose", False)
    analysis_agent = AnalysisAgent(supervisory_config, verbose=verbose)
    
    analysis_results = []
    if state.research_results:
        result = asyncio.run(analysis_agent.analyze_results(state.research_results, state.user_request))
        analysis_results.append(result)
    
    return {
        "analysis_agent_status": AgentStatus.COMPLETED,
        "analysis_results": analysis_results,
        "messages": state.messages + [AIMessage(content="å®Œæˆç»“æœåˆ†æ")]
    }


def synthesize_final_report_node(state: SupervisoryState, config: RunnableConfig) -> Dict[str, Any]:
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘ŠèŠ‚ç‚¹"""
    supervisory_config = Configuration.from_runnable_config(config)
    verbose = config.get("configurable", {}).get("verbose", False)
    synthesis_agent = SynthesisAgent(supervisory_config, verbose=verbose)
    
    final_report = ""
    if state.research_results and state.analysis_results:
        final_report = asyncio.run(synthesis_agent.synthesize_final_report(
            state.research_results,
            state.analysis_results,
            state.user_request
        ))
    
    return {
        "final_synthesis": final_report,
        "messages": state.messages + [AIMessage(content="ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š")]
    }


def route_next_task(state: SupervisoryState, config: RunnableConfig) -> Literal["execute_research", "analyze_results", "synthesize_final_report", "end"]:
    """è·¯ç”±åˆ°ä¸‹ä¸€ä¸ªä»»åŠ¡"""
    if state.deep_researcher_status == AgentStatus.IDLE:
        return "execute_research"
    elif state.deep_researcher_status == AgentStatus.COMPLETED and state.analysis_agent_status == AgentStatus.IDLE:
        return "analyze_results"
    elif state.analysis_agent_status == AgentStatus.COMPLETED:
        return "synthesize_final_report"
    else:
        return "end"


# æ„å»ºä¸»ç®¡æ¶æ„å›¾
def create_supervisory_graph():
    """åˆ›å»ºä¸»ç®¡æ¶æ„å›¾"""
    builder = StateGraph(
        SupervisoryState,
        input=SupervisoryStateInput,
        output=SupervisoryStateOutput,
        config_schema=Configuration,
    )
    
    # æ·»åŠ èŠ‚ç‚¹
    builder.add_node("decompose_request", decompose_request_node)
    builder.add_node("execute_research", execute_research_node)
    builder.add_node("analyze_results", analyze_results_node)
    builder.add_node("synthesize_final_report", synthesize_final_report_node)
    
    # æ·»åŠ è¾¹
    builder.add_edge(START, "decompose_request")
    builder.add_conditional_edges(
        "decompose_request",
        route_next_task,
        {
            "execute_research": "execute_research",
            "analyze_results": "analyze_results",
            "synthesize_final_report": "synthesize_final_report",
            "end": END
        }
    )
    builder.add_conditional_edges(
        "execute_research",
        route_next_task,
        {
            "analyze_results": "analyze_results",
            "synthesize_final_report": "synthesize_final_report",
            "end": END
        }
    )
    builder.add_conditional_edges(
        "analyze_results",
        route_next_task,
        {
            "synthesize_final_report": "synthesize_final_report",
            "end": END
        }
    )
    builder.add_edge("synthesize_final_report", END)
    
    return builder.compile()


# åˆ›å»ºä¸»ç®¡æ¶æ„å›¾å®ä¾‹
supervisory_graph = create_supervisory_graph()


# ä¾¿æ·å‡½æ•°
async def run_supervisory_research(user_request: str, config: Configuration, verbose: bool = False) -> SupervisoryStateOutput:
    """è¿è¡Œä¸»ç®¡æ¶æ„ç ”ç©¶"""
    input_state = SupervisoryStateInput(user_request=user_request)
    config_dict = {
        "configurable": {
            **config.dict(),
            "verbose": verbose
        }
    }
    
    # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨é¿å…äº‹ä»¶å¾ªç¯å†²çª
    result = await supervisory_graph.ainvoke(input_state, config=config_dict)
    return SupervisoryStateOutput(
        final_synthesis=result.get("final_synthesis", ""),
        research_results=result.get("research_results", []),
        analysis_results=result.get("analysis_results", [])
    )
