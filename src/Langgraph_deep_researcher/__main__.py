import argparse
import os
import sys
import time
from pathlib import Path

try:
    # Prefer python-dotenv if available to load .env before reading os.environ
    from dotenv import load_dotenv
except Exception:
    load_dotenv = None

from langgraph.checkpoint.memory import MemorySaver

# Add src to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from Langgraph_deep_researcher.graph import graph
from Langgraph_deep_researcher.state import SummaryStateInput
from Langgraph_deep_researcher.configuration import Configuration


def print_progress(message: str, step: int = None, total: int = None):
    """æ‰“å°è¿›åº¦ä¿¡æ¯"""
    timestamp = time.strftime("%H:%M:%S")
    if step and total:
        print(f"[{timestamp}] æ­¥éª¤ {step}/{total}: {message}")
    else:
        print(f"[{timestamp}] {message}")


def print_step_complete(step_name: str, duration: float):
    """æ‰“å°æ­¥éª¤å®Œæˆä¿¡æ¯"""
    timestamp = time.strftime("%H:%M:%S")
    print(f"[{timestamp}] âœ… {step_name} å®Œæˆ (è€—æ—¶: {duration:.1f}ç§’)")


def print_research_progress(state, step_name: str):
    """æ‰“å°ç ”ç©¶è¿›åº¦ä¿¡æ¯"""
    timestamp = time.strftime("%H:%M:%S")
    
    # å¤„ç†çŠ¶æ€å¯èƒ½æ˜¯å­—å…¸æˆ–å¯¹è±¡çš„æƒ…å†µ
    if isinstance(state, dict):
        loop_count = state.get('research_loop_count', 0)
        sources_gathered = state.get('sources_gathered', [])
        running_summary = state.get('running_summary', '')
    else:
        loop_count = getattr(state, 'research_loop_count', 0)
        sources_gathered = getattr(state, 'sources_gathered', [])
        running_summary = getattr(state, 'running_summary', '')
    
    sources_count = len(sources_gathered) if sources_gathered else 0
    summary_length = len(running_summary) if running_summary else 0
    
    print(f"[{timestamp}] ğŸ” {step_name}")
    print(f"    ğŸ“Š ç ”ç©¶å¾ªç¯: {loop_count}, æ¥æºæ•°é‡: {sources_count}, æ‘˜è¦é•¿åº¦: {summary_length} å­—ç¬¦")


def main():
    # Load environment variables from nearest .env if python-dotenv is available
    # This allows LOCAL_LLMã€LLM_PROVIDER ç­‰åœ¨ CLI æ¨¡å¼ä¸‹ç”Ÿæ•ˆ
    if load_dotenv is not None:
        # Search from CWD upwards to project root for a .env
        # This mimics common tooling behavior and is harmless if not found
        cwd = Path.cwd()
        env_path = None
        for parent in [cwd, *cwd.parents]:
            candidate = parent / ".env"
            if candidate.exists():
                env_path = candidate
                break
        if env_path is not None:
            load_dotenv(dotenv_path=str(env_path), override=False)
    parser = argparse.ArgumentParser(
        prog="Langgraph_deep_researcher",
        description="Run local deep researcher from CLI and write output to a file.",
    )
    parser.add_argument(
        "--topic",
        required=True,
        help="Research topic to investigate",
    )
    parser.add_argument(
        "--out",
        required=True,
        help="Output file path for the final markdown summary",
    )
    parser.add_argument(
        "--loops",
        type=int,
        default=None,
        help="Override max research loops (optional)",
    )
    parser.add_argument(
        "--provider",
        choices=["ollama", "openai"],
        default=None,
        help="Override LLM provider (optional)",
    )
    parser.add_argument(
        "--model",
        default=None,
        help="Override local model name (optional)",
    )
    parser.add_argument(
        "--search",
        choices=["duckduckgo", "tavily", "perplexity", "searxng"],
        default=None,
        help="Override search API (optional)",
    )
    parser.add_argument(
        "--tool-calling",
        action="store_true",
        default=None,
        help="Use tool calling instead of JSON mode",
    )
    parser.add_argument(
        "--no-strip-think",
        action="store_true",
        default=None,
        help="Do not strip <think> tokens from model responses",
    )

    args = parser.parse_args()

    # Prepare configurable overrides. Only include keys explicitly set.
    configurable_overrides = {}
    if args.loops is not None:
        configurable_overrides["max_web_research_loops"] = args.loops
    if args.provider is not None:
        configurable_overrides["llm_provider"] = args.provider
    if args.model is not None:
        configurable_overrides["local_llm"] = args.model
    if args.search is not None:
        configurable_overrides["search_api"] = args.search
    if args.tool_calling is not None:
        configurable_overrides["use_tool_calling"] = True
    if args.no_strip_think is not None:
        configurable_overrides["strip_thinking_tokens"] = False

    # Build RunnableConfig expected by the graph
    runnable_config = {"configurable": configurable_overrides} if configurable_overrides else {}

    # Optional in-memory checkpointer; not required but helpful for consistency
    # Note: graph compiled in module already; we can use .invoke

    # æ˜¾ç¤ºå¼€å§‹ä¿¡æ¯
    print_progress(f"ğŸš€ å¼€å§‹ç ”ç©¶ä¸»é¢˜: {args.topic}")
    print_progress(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {args.out}")
    if configurable_overrides:
        print_progress(f"âš™ï¸ é…ç½®è¦†ç›–: {configurable_overrides}")
    
    start_time = time.time()
    
    # Run the graph with streaming to show progress
    input_state = SummaryStateInput(research_topic=args.topic)
    
    # ä½¿ç”¨æµå¼æ‰§è¡Œæ¥æ˜¾ç¤ºè¿›åº¦
    print_progress("ğŸ”„ æ­£åœ¨æ‰§è¡Œç ”ç©¶æµç¨‹...")
    
    # è·Ÿè¸ªæ­¥éª¤
    step_count = 0
    total_steps = 5  # generate_query, web_research, summarize_sources, reflect_on_summary, finalize_summary
    
    try:
        # ä½¿ç”¨ stream æ–¹æ³•æ¥è·å–å®æ—¶è¿›åº¦
        for chunk in graph.stream(input_state, config=runnable_config):
            step_count += 1
            node_name = list(chunk.keys())[0] if chunk else "unknown"
            state = list(chunk.values())[0] if chunk else None
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºçŠ¶æ€ç±»å‹å’Œå†…å®¹
            if state:
                print_progress(f"ğŸ” è°ƒè¯•: èŠ‚ç‚¹={node_name}, çŠ¶æ€ç±»å‹={type(state)}")
                if isinstance(state, dict):
                    print_progress(f"    ğŸ“‹ çŠ¶æ€å­—æ®µ: {list(state.keys())}")
                else:
                    print_progress(f"    ğŸ“‹ çŠ¶æ€å±æ€§: {dir(state)[:5]}...")
            
            # æ ¹æ®èŠ‚ç‚¹åç§°æ˜¾ç¤ºä¸åŒçš„è¿›åº¦ä¿¡æ¯
            if node_name == "generate_query":
                print_progress("ğŸ“ ç”Ÿæˆæœç´¢æŸ¥è¯¢", step_count, total_steps)
            elif node_name == "web_research":
                print_research_progress(state, "ğŸŒ æ‰§è¡Œç½‘ç»œæœç´¢")
            elif node_name == "summarize_sources":
                print_research_progress(state, "ğŸ“‹ æ€»ç»“ä¿¡æ¯æ¥æº")
            elif node_name == "reflect_on_summary":
                print_research_progress(state, "ğŸ¤” åæ€ç ”ç©¶è¿›åº¦")
            elif node_name == "finalize_summary":
                print_progress("ğŸ“„ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š", step_count, total_steps)
            
            # çŸ­æš‚å»¶è¿Ÿä»¥ä¾¿ç”¨æˆ·çœ‹åˆ°è¿›åº¦
            time.sleep(0.1)
        
        # è·å–æœ€ç»ˆç»“æœ
        result = graph.invoke(input_state, config=runnable_config)
        
    except Exception as e:
        print_progress(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        sys.exit(1)

    summary = result.get("running_summary")
    if not summary:
        print_progress("âŒ é”™è¯¯: å›¾æ‰§è¡Œæœªäº§ç”Ÿè¿è¡Œæ‘˜è¦")
        sys.exit(1)

    # æ˜¾ç¤ºå®Œæˆä¿¡æ¯
    total_time = time.time() - start_time
    print_step_complete("ç ”ç©¶æµç¨‹", total_time)
    
    # Ensure directory exists
    out_path = os.path.abspath(args.out)
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    # å†™å…¥æ–‡ä»¶
    print_progress(f"ğŸ’¾ æ­£åœ¨å†™å…¥æ–‡ä»¶: {out_path}")
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(summary)

    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    summary_length = len(summary) if summary else 0
    sources_gathered = result.get("sources_gathered", [])
    sources_count = len(sources_gathered) if sources_gathered else 0
    print_progress(f"ğŸ“Š ç ”ç©¶å®Œæˆç»Ÿè®¡:")
    print_progress(f"   ğŸ“„ æŠ¥å‘Šé•¿åº¦: {summary_length:,} å­—ç¬¦")
    print_progress(f"   ğŸ”— ä¿¡æ¯æ¥æº: {sources_count} ä¸ª")
    print_progress(f"   â±ï¸ æ€»è€—æ—¶: {total_time:.1f} ç§’")
    print(f"âœ… ç ”ç©¶æ‘˜è¦å·²ä¿å­˜è‡³: {out_path}")


if __name__ == "__main__":
    main()


