"""
ä¸»ç®¡æ¶æ„ç³»ç»Ÿå‘½ä»¤è¡Œæ¥å£
"""

import asyncio
import argparse
import sys
import os
from pathlib import Path

try:
    # Prefer python-dotenv if available to load .env before reading os.environ
    from dotenv import load_dotenv
except Exception:
    load_dotenv = None

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from Langgraph_deep_researcher.supervisory_architecture import (
    supervisory_graph,
    run_supervisory_research
)
from Langgraph_deep_researcher.configuration import Configuration, SearchAPI


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="ä¸»ç®¡æ¶æ„ç ”ç©¶ç³»ç»Ÿ - å¤š Agent åä½œç ”ç©¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python -m Langgraph_deep_researcher.supervisory_cli "äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿"
  python -m Langgraph_deep_researcher.supervisory_cli --topic "é‡å­è®¡ç®—åº”ç”¨" --out report.md
  python -m Langgraph_deep_researcher.supervisory_cli --provider openai --model gpt-4 "åŒ»ç–—AIç ”ç©¶"
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument(
        "topic",
        nargs="?",
        help="ç ”ç©¶ä¸»é¢˜"
    )
    
    # å¯é€‰å‚æ•°
    parser.add_argument(
        "--topic", "-t",
        dest="research_topic",
        help="ç ”ç©¶ä¸»é¢˜ (ä¸ä½ç½®å‚æ•°ç›¸åŒ)"
    )
    
    parser.add_argument(
        "--out", "-o",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: supervisory_research_report.md)"
    )
    
    parser.add_argument(
        "--provider",
        choices=["ollama", "openai"],
        default="ollama",
        help="LLM æä¾›å•† (é»˜è®¤: ollama)"
    )
    
    parser.add_argument(
        "--model",
        default="llama3",
        help="LLM æ¨¡å‹åç§° (é»˜è®¤: llama3)"
    )
    
    parser.add_argument(
        "--search-api",
        choices=["duckduckgo", "tavily", "perplexity", "searxng"],
        default="duckduckgo",
        help="æœç´¢å¼•æ“ API (é»˜è®¤: duckduckgo)"
    )
    
    parser.add_argument(
        "--max-loops",
        type=int,
        default=3,
        help="æœ€å¤§ç ”ç©¶å¾ªç¯æ¬¡æ•° (é»˜è®¤: 3)"
    )
    
    parser.add_argument(
        "--ollama-url",
        default="http://localhost:11434",
        help="Ollama æœåŠ¡åœ°å€ (é»˜è®¤: http://localhost:11434)"
    )
    
    parser.add_argument(
        "--openai-url",
        default="https://api.openai.com/v1",
        help="OpenAI API åœ°å€ (é»˜è®¤: https://api.openai.com/v1)"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†è¾“å‡º"
    )
    
    return parser


def print_progress(message: str, verbose: bool = False):
    """æ‰“å°è¿›åº¦ä¿¡æ¯"""
    # åœ¨verboseæ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼Œå¦åˆ™åªæ˜¾ç¤ºå…³é”®è¿›åº¦
    if verbose or "å¼€å§‹æ‰§è¡Œ" in message or "ç ”ç©¶å®Œæˆ" in message or "æŠ¥å‘Šå·²ä¿å­˜" in message:
        print(f"ğŸ”„ {message}")


async def run_supervisory_research_cli(args):
    """è¿è¡Œä¸»ç®¡æ¶æ„ç ”ç©¶"""
    # ç¡®å®šç ”ç©¶ä¸»é¢˜
    topic = args.topic or args.research_topic
    if not topic:
        print("âŒ é”™è¯¯: è¯·æä¾›ç ”ç©¶ä¸»é¢˜")
        return False
    
    # åˆ›å»ºé…ç½® - ä¼˜å…ˆçº§: .envæ–‡ä»¶ > å‘½ä»¤è¡Œå‚æ•° > é»˜è®¤å€¼
    # ä»ç¯å¢ƒå˜é‡è·å–é»˜è®¤å€¼ï¼ˆ.envæ–‡ä»¶å·²åŠ è½½ï¼‰
    default_config = Configuration.from_runnable_config()
    
    # å‡†å¤‡é…ç½®è¦†ç›–å‚æ•° - åªæœ‰æ˜ç¡®æŒ‡å®šçš„å‘½ä»¤è¡Œå‚æ•°æ‰è¦†ç›–
    config_overrides = {}
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†é»˜è®¤å€¼ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„å€¼
    if args.provider == "ollama" and hasattr(default_config, 'llm_provider') and default_config.llm_provider != "ollama":
        # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„å€¼
        pass  # ä¸è¦†ç›–ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
    elif args.provider != "ollama":
        # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†éé»˜è®¤å€¼
        config_overrides["llm_provider"] = args.provider
    
    if args.model == "llama3" and hasattr(default_config, 'local_llm') and default_config.local_llm != "llama3":
        # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„å€¼
        pass  # ä¸è¦†ç›–ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
    elif args.model != "llama3":
        # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†éé»˜è®¤å€¼
        config_overrides["local_llm"] = args.model
    
    if args.search_api == "duckduckgo" and hasattr(default_config, 'search_api') and default_config.search_api != "duckduckgo":
        # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„å€¼
        pass  # ä¸è¦†ç›–ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
    elif args.search_api != "duckduckgo":
        # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†éé»˜è®¤å€¼
        config_overrides["search_api"] = args.search_api
    
    if args.max_loops == 3 and hasattr(default_config, 'max_web_research_loops') and default_config.max_web_research_loops != 3:
        # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„å€¼
        pass  # ä¸è¦†ç›–ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
    elif args.max_loops != 3:
        # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†éé»˜è®¤å€¼
        config_overrides["max_web_research_loops"] = args.max_loops
    
    if args.ollama_url == "http://localhost:11434" and hasattr(default_config, 'ollama_base_url') and default_config.ollama_base_url != "http://localhost:11434":
        # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„å€¼
        pass  # ä¸è¦†ç›–ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
    elif args.ollama_url != "http://localhost:11434":
        # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†éé»˜è®¤å€¼
        config_overrides["ollama_base_url"] = args.ollama_url
    
    if args.openai_url == "https://api.openai.com/v1" and hasattr(default_config, 'openai_base_url') and default_config.openai_base_url != "https://api.openai.com/v1":
        # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„å€¼
        pass  # ä¸è¦†ç›–ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
    elif args.openai_url != "https://api.openai.com/v1":
        # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†éé»˜è®¤å€¼
        config_overrides["openai_base_url"] = args.openai_url
    
    # åˆ›å»ºæœ€ç»ˆé…ç½®
    config = Configuration(**{**default_config.model_dump(), **config_overrides})
    
    print(f"ğŸ¯ ç ”ç©¶ä¸»é¢˜: {topic}")
    print(f"ğŸ¤– LLM æä¾›å•†: {config.llm_provider}")
    print(f"ğŸ§  æ¨¡å‹: {config.local_llm}")
    print(f"ğŸ” æœç´¢å¼•æ“: {config.search_api}")
    print(f"ğŸ”„ æœ€å¤§å¾ªç¯æ¬¡æ•°: {config.max_web_research_loops}")
    print("=" * 50)
    
    try:
        print_progress("å¼€å§‹æ‰§è¡Œä¸»ç®¡æ¶æ„ç ”ç©¶æµç¨‹...", args.verbose)
        
        # è¿è¡Œç ”ç©¶
        result = await run_supervisory_research(topic, config, verbose=args.verbose)
        
        print("âœ… ç ”ç©¶å®Œæˆ!")
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶
        output_file = args.out or "supervisory_research_report.md"
        
        # ä¿å­˜ç»“æœ
        with open(output_file, "w", encoding="utf-8") as f:
            f.write("# ä¸»ç®¡æ¶æ„ç ”ç©¶æŠ¥å‘Š\n\n")
            f.write(f"**ç ”ç©¶ä¸»é¢˜**: {topic}\n\n")
            f.write(f"**LLM æä¾›å•†**: {config.llm_provider}\n")
            f.write(f"**æ¨¡å‹**: {config.local_llm}\n")
            f.write(f"**æœç´¢å¼•æ“**: {config.search_api}\n")
            f.write(f"**æœ€å¤§å¾ªç¯æ¬¡æ•°**: {config.max_web_research_loops}\n\n")
            
            f.write("## ç ”ç©¶ç»“æœ\n\n")
            for i, research_result in enumerate(result.research_results, 1):
                f.write(f"### ç ”ç©¶ç»“æœ {i}\n\n")
                f.write(research_result)
                f.write("\n\n")
            
            f.write("## åˆ†æç»“æœ\n\n")
            for i, analysis_result in enumerate(result.analysis_results, 1):
                f.write(f"### åˆ†æç»“æœ {i}\n\n")
                f.write(analysis_result)
                f.write("\n\n")
            
            f.write("## æœ€ç»ˆç»¼åˆæŠ¥å‘Š\n\n")
            f.write(result.final_synthesis)
        
        print(f"ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºæ‘˜è¦
        if args.verbose:
            print("\nğŸ“Š ç ”ç©¶æ‘˜è¦:")
            print(f"- ç ”ç©¶ç»“æœæ•°é‡: {len(result.research_results)}")
            print(f"- åˆ†æç»“æœæ•°é‡: {len(result.analysis_results)}")
            print(f"- æœ€ç»ˆæŠ¥å‘Šé•¿åº¦: {len(result.final_synthesis)} å­—ç¬¦")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    # Load environment variables from nearest .env if python-dotenv is available
    # This allows LOCAL_LLMã€LLM_PROVIDER ç­‰åœ¨ CLI æ¨¡å¼ä¸‹ç”Ÿæ•ˆ
    if load_dotenv is not None:
        # Search from CWD upwards to project root for a .env
        # This mimics common tooling behavior and is harmless if not found
        cwd = Path.cwd()
        env_path = None
        for parent in [cwd, *cwd.parents]:
            candidate = parent / ".env"
            if candidate.exists():
                env_path = candidate
                break
        if env_path is not None:
            load_dotenv(dotenv_path=str(env_path), override=False)
            print(f"ğŸ“ å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_path}")
    
    parser = create_parser()
    args = parser.parse_args()
    
    # è¿è¡Œå¼‚æ­¥ç ”ç©¶
    success = asyncio.run(run_supervisory_research_cli(args))
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
